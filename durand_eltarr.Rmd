---
header-includes:
- \usepackage{stmaryrd}
- \usepackage{amsfonts}
- \usepackage{amsmath}
output:
  pdf_document: default
  html_document:
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

```

\begin{titlepage}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\center
\textsc{\LARGE
STA212 - Méthodes de rééchantillonnage} \\[2cm]
\LARGE Enseignant: Mohammed Sedki  \\[2cm]

\HRule \\[0.4cm]
{ \huge \bfseries Devoir : aspects pratiques \\[0.15cm] }
\HRule \\[3cm] \LARGE
Romin DURAND \\
Loukman Eltarr
\\[3cm]
\today \\ [1cm]
\end{titlepage}

# Arbre de décision unique

```{r}
setwd('~/Cours/STA212/STA212DM')
#setwd("/home/lokmen/Documents/ENSTA/STAT/STA212/STA212DM")
rm(list = objects())
graphics.off()
OJ=read.csv("oj.csv", header = TRUE)
#View(OJ)
```

On regarde la nature de nos données. On a 1070 observations pour 18 variables différentes. Les variables categorielles sont $\texttt{Purchase}$ qui admet deux niveaux, et $\texttt{Store 7}$ qui admet aussi deux niveaux. Les autres sont numériques.
```{r}
str(OJ) 
```

## Analyse Univariée
On procéde à une analyse univariée des variables. On se sert de la description des variables ainsi que des commandes $\texttt{summary}$, $\texttt{plot}$ et $\texttt{table}$.

Par exemple, on peut voir que les variables $\texttt{SpecialCH}$ et $\texttt{SpecialMM}$ prennent seulement les valeurs 0 et 1.
```{r}
table(OJ$SpecialCH)
plot(OJ$SpecialCH)
```



```{r}
table(OJ$SpecialMM)
plot(OJ$SpecialMM)
```

De la même manière $\texttt{STORE}$ ne prend que les valeurs entre 0 et 4.
```{r}
table(OJ$STORE)
plot(OJ$STORE)
```

On préfère alors les transformer en variables catégorielles:

```{r}
OJ$SpecialMM <- as.factor(OJ$SpecialMM)
OJ$SpecialCH <- as.factor(OJ$SpecialCH)
OJ$STORE <- as.factor(OJ$STORE+1) ## On préfère avoir des valeurs entre 1 et 5.
```
On regarde la proportion de "MM" par rapport à celle de "CH". Il ya plus de CH que de MM qui ont été commendés. La proportion est de 61%-39%.
```{r}
table(OJ$Purchase)/nrow(OJ)
plot(OJ$Purchase, main ="CH VS MM in Purchase",col=c("darkorange","darkolivegreen3"))
```
## Question 1
On divise d'abord notre jeu de donnée en une partie **train** et une partie **test** 
```{r}
set.seed(2) ## On précise la graine afin d'avoir la reproductibilité
size_sample=800 ## tailer de notre echantillon train
train <- sample(c(1:nrow(OJ)), size=floor(size_sample)) ## list comportant l'index de la partie train
```

## Question 2
On va utiliser l'apprentissage par arbre de décision pour faire un prédction sur la partie test à partir de la partie train. On utilise d'baord un arbre obtenu sans élagage puis un autre avec élagage, on comparera ensuite la qualité des deux prédictions.

```{r message=FALSE}
require(rpart)
require(rpart.plot)
```

```{r}
## Arbre sans élagage
#   # Avec minsplit =1 on continue à explorer un noeau tant qu'il y a plus d'une seule variable 
#   # cp = 0 indique qu'on explore un noeau même si il n'apporte pas de précision supplémentaire 
#   # xval= 10 pour le nombre de fold dans notre validation croisée
three.0 <- rpart(Purchase~.,
                data=OJ[train,], 
                control=rpart.control(minsplit=1,cp=0, xval=10),model =TRUE)
rpart.plot(three.0)

## Arbre avec élagage
three.1 <- prune(three.0, cp = three.0$cptable[which.min(three.0$cptable[,"xerror"]),"CP"],model =TRUE)
rpart.plot(three.1)
```

On voit que le graphique obtenu sans élagage est difficilement lisible ou interpretable. Il présente aussi probablement un problèmes d'affichage au vu du de la trop importante information qu'il contient. Au contraire, pour l'arbre pourlequel on a utilisé l'élagage, on voit clairement les noeuds internees et les feuilles ainsi que les proportions associées.
### Erreurs de Prédiction
On compare ensuite leurs erreurs de prediction.
```{r}
pred.0 <- predict(three.0, OJ, type="class")
mean(OJ$Purchase[-train]!=pred.0[-train])

pred.1 <- predict(three.1, OJ, type="class")
mean(OJ$Purchase[-train]!=pred.1[-train])
```
On note que notre taux d'erreur est autour de 0.21 pour le premier arbre. Ell est légerement inférieur à 0.16 pour le deuxième. Le second est donc un légèrement meilleur. 

### Matrice de Confusion
On dresse alors une matrice de confusion. On a :
* Pour l'abre sans élagage : 
  * 142 CH qui ont correctement été prédits.
  * 31 CH qui ont été incorrectement prédits.
  * 26 MM qui ont été incorrectement prédits.
  * 71 MM qui ont correctement été prédits.
Globalement la prediction est assez bonne. 
```{r}
table(OJ$Purchase[-train],pred.0[-train],dnn = c("Purchase", "Prediction"))
```
* Pour l'abre sans élagage : 
  * 151 CH qui ont correctement été prédits.
  * 22 CH qui ont été incorrectement prédits.
  * 21 MM qui ont été incorrectement prédits.
  * 77 MM qui ont correctement été prédits.
Globalement la prediction est assez bonne. 
```{r}
table(OJ$Purchase[-train],pred.1[-train],dnn = c("Purchase", "Prediction"))
```

## Question 3
```{r message=FALSE}
rpart.plot(three.1)
```

On peut observer l'importance relative des variables.
```{r message=FALSE}
three.1$variable.importance
```

```{r message=FALSE}
require(lattice)
barchart(three.1$variable.importance,xlab = "Variables", ylab = "Importance", main="Impotance des Variables",col=c('cornflowerblue','chocolate3','aquamarine2','brown2','burlywood3','slategray2','khaki4','peru','skyblue2','violet','darkorange1','gold2','lightpink4','seagreen4'))
```

# Forêt aléatoires


```{r echo = FALSE, results=FALSE, warning=FALSE, include = FALSE}
setwd('~/Cours/STA212/STA212DM')
#setwd("/home/lokmen/Documents/ENSTA/STAT/STA212/STA212DM")
rm(list = objects())
graphics.off()
library(readr)
email <- read_csv("email.csv")
View(email)
```

Il faut tout d'abord changer la variable Class en variable factor :

```{r}
email$Class = as.factor(email$Class)
```


## Question 4

```{r message=FALSE, warning=FALSE}
require(rpart)
require(rpart.plot)
require(ipred)
require(caret)
require(randomForest)
require(doParallel)
```

```{r results=FALSE}
N = nrow(email)
set.seed(103)
train = sample(1:N, round(0.75*N))
email.tr = email[train,]
email.te = email[-train,]
```

```{r echo = FALSE, results=FALSE}
summary(email)
```


```{r echo = FALSE, results=FALSE, include = FALSE}
library(corrplot)
corrplot(cor(email[,-58]), tl.pos="n")
```

Ajustons tout d'abord un arbre sans élagage :

```{r}
cart.0 <- rpart(Class~.,
                data=email.tr, 
                control=rpart.control(minsplit=1,cp=0, xval=30)) # minsplit=1,cp=0
                                                                # pour avoir un arbre le
                                                                # plus profond possible
rpart.plot(cart.0)

```

Puis élagons cette arbre :

```{r}
cart.pruned <- prune(cart.0, cp = cart.0$cptable[which.min(cart.0$cptable[,"xerror"]),"CP"])
rpart.plot(cart.pruned)
```

Enfin on calcul l'erreur de test (taux de mauvais classement), en appliquant la règle de Bayes :

```{r}
pred.pruned <- predict(cart.pruned, email.te)
mean(abs(ifelse(email.te$Class == "Spam", 1,0) - ifelse(pred.pruned[,2] >.5, 1,0)))

```

Nous atteignons donc un taux de mauvais classement de 8% avec ce modèle. 

Pour 

## Question 5

Nous réalisons un bagging 100 arbres de décisions à l'aide de la fonction bagging de la librairie ipred :

```{r}
bag.email <- bagging(Class~., data=email.tr, mfinal=100)

```

Puis on peut évaluer l'erreur de test :

```{r}
pred.bag <- predict(bag.email, email.te)
mean(abs(ifelse(email.te$Class == "Spam", 1,0) - ifelse(pred.bag == "Spam", 1,0)))
```


Nous avons donc une erreur de test de 7,5%.



## Question 6

Enfin, nous ajustons un modèle random forrest à 100 arbres en choisisant le mtry, (c'est à dire le nombre de variable que l'on prend pour chaque arbre), par validation croisée.

```{r eval=FALSE}
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

control <- trainControl(method="repeatedcv", number=5, repeats=10)
rfGrid <-  expand.grid(mtry = 50:57)
RFmodel <- train(Class~., data=email.tr, method="rf", 
                 trControl=control,
                 ntree=100, 
                 tuneGrid = rfGrid,
                 verbose=FALSE)
stopCluster(cl)
plot(RFmodel)

```

Le choix de mtry est reproductible grâce à la validation croisée.

Puis on peut évaluer l'erreur de test :
```{r eval=FALSE}
pred.rf <- predict(RFmodel, email.te)
mean(abs(ifelse(email.te$Class == "Spam", 1,0) - ifelse(pred.rf == "Spam", 1,0)))

```

On a une erreur de test de 7%.

## Question 7



# Autour de l’algorithme Adaboost

## Question 10


\begin{tabular}{|l|c|c|c|r|} \hline & &  \textbf{Perte}  & &  \\ & & \textbf{exponentielle} & \textbf{Perte binaire} & \\ $\hat{c}(x_*)$ & $\hat{y}_*$ & $exp(-y_* \hat{c}(x_*))$ &   $1(y_* \neq \hat{y}_*)$ & $y_*$ \tabularnewline \hline 0.3 & & & & -1 \tabularnewline \hline -0.2 & & & & -1 \tabularnewline \hline 1.5 & & & & 1 \tabularnewline \hline -4.3 & & & & 1 \tabularnewline \hline \end{tabular}










