---
header-includes:
- \usepackage{stmaryrd}
- \usepackage{amsfonts}
- \usepackage{amsmath}
output:
  html_document:
    df_print: paged
  pdf_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

```

\begin{titlepage}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\center
\textsc{\LARGE
STA212 - Méthodes de rééchantillonnage} \\[2cm]
\LARGE Enseignant: Mohammed Sedki  \\[2cm]

\HRule \\[0.4cm]
{ \huge \bfseries Devoir : aspects pratiques \\[0.15cm] }
\HRule \\[3cm] \LARGE
Romin DURAND \\
Loukman Eltarr
\\[3cm]
\today \\ [1cm]
\end{titlepage}

# Arbre de décision unique

```{r}
#setwd('~/Cours/STA212/STA212DM')
setwd("/home/lokmen/Documents/ENSTA/STAT/STA212/STA212DM")
rm(list = objects())
graphics.off()
OJ=read.csv("oj.csv", header = TRUE)
#View(OJ)
```

On regarde la nature de nos données. On a 1070 observations pour 18 variables différentes. Les variables categorielles sont $\texttt{Purchase}$ qui admet deux niveaux, et $\texttt{Store 7}$ qui admet aussi deux niveaux. Les autres sont numériques.
```{r}
str(OJ) 
```

## Analyse Univariée
On procéde à une analyse univariée des variables. On se sert de la description des variables ainsi que des commandes $\texttt{summary}$, $\texttt{plot}$ et $\texttt{table}$.

Par exemple, on peut voir que les variables $\texttt{SpecialCH}$ et $\texttt{SpecialMM}$ prennent seulement les valeurs 0 et 1.
```{r fig1, fig.height=4}
table(OJ$SpecialCH)
plot(OJ$SpecialCH)
```



```{r fig2,  fig.height=4}
table(OJ$SpecialMM)
plot(OJ$SpecialMM)
```

De la même manière $\texttt{STORE}$ ne prend que les valeurs entre 0 et 4.
```{r fig3, fig.height=4}
table(OJ$STORE)
plot(OJ$STORE)
```

On préfère alors les transformer en variables catégorielles:

```{r}
OJ$SpecialMM <- as.factor(OJ$SpecialMM)
OJ$SpecialCH <- as.factor(OJ$SpecialCH)
OJ$STORE <- as.factor(OJ$STORE+1) ## On préfère avoir des valeurs entre 1 et 5.
```
On regarde la proportion de "MM" par rapport à celle de "CH". Il ya plus de CH que de MM qui ont été commendés. La proportion est de 61%-39%.
```{r}
table(OJ$Purchase)/nrow(OJ)
plot(OJ$Purchase, main ="CH VS MM in Purchase",col=c("darkorange","darkolivegreen3"))
```

## Question 1
On divise d'abord notre jeu de donnée en une partie **train** et une partie **test** 
```{r}
set.seed(2) ## On précise la graine afin d'avoir la reproductibilité
size_sample=800 ## tailer de notre echantillon train
train <- sample(c(1:nrow(OJ)), size=floor(size_sample)) ## liste comportant l'index de la partie train
```

## Question 2
On va utiliser l'apprentissage par arbre de décision pour faire un prédction sur la partie test à partir de la partie train. On utilise d'baord un arbre obtenu sans élagage puis un autre avec élagage, on comparera ensuite la qualité des deux prédictions.

```{r message=FALSE}
require(rpart)
require(rpart.plot)
```

```{r}
## Arbre sans élagage
#   # Avec minsplit =1 on continue à explorer un noeau tant qu'il y a plus d'une seule variable 
#   # cp = 0 indique qu'on explore un noeau même si il n'apporte pas de précision supplémentaire 
#   # xval= 10 pour le nombre de fold dans notre validation croisée
three.0 <- rpart(Purchase~.,
                data=OJ[train,], 
                control=rpart.control(minsplit=1,cp=0, xval=10),model =TRUE)
rpart.plot(three.0)

## Arbre avec élagage
three.1 <- prune(three.0, cp = three.0$cptable[which.min(three.0$cptable[,"xerror"]),"CP"],model =TRUE)
rpart.plot(three.1)
```

On voit que le graphique obtenu sans élagage est difficilement lisible ou interpretable. Il présente aussi probablement un problèmes d'affichage au vu du de la trop importante information qu'il contient. Au contraire, pour l'arbre pourlequel on a utilisé l'élagage, on voit clairement les noeuds internees et les feuilles ainsi que les proportions associées.

### Erreurs de Prédiction

On compare ensuite leurs erreurs de prediction.
```{r}
pred.0 <- predict(three.0, OJ, type="class")
mean(OJ$Purchase[-train]!=pred.0[-train])

pred.1 <- predict(three.1, OJ, type="class")
mean(OJ$Purchase[-train]!=pred.1[-train])
```
On note que notre taux d'erreur est autour de 0.21 pour le premier arbre. Ell est légerement inférieur à 0.16 pour le deuxième. Le second est donc un légèrement meilleur. 

### Matrice de Confusion

On dresse alors une matrice de confusion. On a :

* Pour l'abre sans élagage : 
  * 142 CH qui ont correctement été prédits.
  * 31 CH qui ont été incorrectement prédits.
  * 26 MM qui ont été incorrectement prédits.
  * 71 MM qui ont correctement été prédits.
Globalement la prediction est assez bonne. 

```{r}
table(OJ$Purchase[-train],pred.0[-train],dnn = c("Purchase", "Prediction"))
```

* Pour l'abre sans élagage : 
  * 151 CH qui ont correctement été prédits.
  * 22 CH qui ont été incorrectement prédits.
  * 21 MM qui ont été incorrectement prédits.
  * 77 MM qui ont correctement été prédits.
Globalement la prediction est assez bonne. 

```{r}
table(OJ$Purchase[-train],pred.1[-train],dnn = c("Purchase", "Prediction"))
```

## Question 3
Ici on commente l'apparition de la feuille la plus à gauche.
On peut voir que depuis la racine il ya une séparation selon la valeur de $\texttt{LoyalCH}$ par deux fois. En effet, l'algorithme a d'abord distingué les valeurs supérieures et inférieures à 0.5 puis identiquement pour la valeurs 0.74. La zone correspondant à $\texttt{LoyalCH}\,>\, 0.74$ contient donc 34% des obseevations.
```{r message=FALSE}
rpart.plot(three.1)
```

On peut observer l'importance relative des variables.
```{r message=FALSE}
three.1$variable.importance
```

```{r message=FALSE}
require(lattice)
barchart(three.1$variable.importance,xlab = "Variables", ylab = "Importance", main="Impotance des Variables",col=c('cornflowerblue','chocolate3','aquamarine2','brown2','burlywood3','slategray2','khaki4','peru','skyblue2','violet','darkorange1','gold2','lightpink4','seagreen4'))
```

# Forêt aléatoires


```{r echo = FALSE, results=FALSE, warning=FALSE, include = FALSE}
#setwd('~/Cours/STA212/STA212DM')
setwd("/home/lokmen/Documents/ENSTA/STAT/STA212/STA212DM")
rm(list = objects())
graphics.off()
library(readr)
email <- read_csv("email.csv")
#View(email)
```

Il faut tout d'abord changer la variable Class en variable factor :

```{r}
email$Class = as.factor(email$Class)
```


## Question 4

```{r message=FALSE, warning=FALSE}
require(rpart)
require(rpart.plot)
require(ipred)
require(caret)
require(randomForest)
require(doParallel)
require(JOUSBoost)
require(xgboost)
```

```{r results=FALSE}
N = nrow(email)
set.seed(103)
train = sample(1:N, round(0.75*N))
email.tr = email[train,]
email.te = email[-train,]
```

```{r echo = FALSE, results=FALSE}
summary(email)
```


```{r echo = FALSE, results=FALSE, include = FALSE}
library(corrplot)
corrplot(cor(email[,-58]), tl.pos="n")
```

Ajustons tout d'abord un arbre sans élagage :

```{r}
cart.0 <- rpart(Class~.,
                data=email.tr, 
                control=rpart.control(minsplit=1,cp=0, xval=30)) # minsplit=1,cp=0
                                                                # pour avoir un arbre le
                                                                # plus profond possible
rpart.plot(cart.0)

```

Puis élagons cet arbre :

```{r}
cart.pruned <- prune(cart.0, cp = cart.0$cptable[which.min(cart.0$cptable[,"xerror"]),"CP"])
rpart.plot(cart.pruned)
```

Enfin on calcul l'erreur de test (taux de mauvais classement), en appliquant la règle de Bayes :

```{r}
pred.pruned <- predict(cart.pruned, email.te)
mean(abs(ifelse(email.te$Class == "Spam", 1,0) - ifelse(pred.pruned[,2] >.5, 1,0)))

```

Nous atteignons donc un taux de mauvais classement de 8% avec ce modèle. 


## Question 5

Nous réalisons un bagging 100 arbres de décisions à l'aide de la fonction bagging de la librairie ipred :

```{r}
bag.email <- bagging(Class~., data=email.tr, mfinal=100)

```

Puis on peut évaluer l'erreur de test :

```{r}
pred.bag <- predict(bag.email, email.te)
mean(abs(ifelse(email.te$Class == "Spam", 1,0) - ifelse(pred.bag == "Spam", 1,0)))
```


Nous avons donc une erreur de test de 7,5%.



## Question 6

Enfin, nous ajustons un modèle random forest à 100 arbres en choisisant le mtry, (c'est à dire le nombre de variable que l'on prend pour chaque arbre), par validation croisée.

```{r cache = TRUE}
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

control <- trainControl(method="repeatedcv", number=5, repeats=10)
rfGrid <-  expand.grid(mtry = 1:20)
RFmodel <- train(Class~., data=email.tr, method="rf", 
                 trControl=control,
                 ntree=100, 
                 tuneGrid = rfGrid,
                 verbose=FALSE)
stopCluster(cl)
plot(RFmodel)

```

Le choix de mtry fait par validation croisée n'est pas reproductible en pratique. Cela demande trop de temps de calcul pour réactualiser le mtry à chaque fois que les donnée changent.

Puis on peut évaluer l'erreur de test :
```{r cache = TRUE}
pred.rf <- predict(RFmodel, email.te)
mean(abs(ifelse(email.te$Class == "Spam", 1,0) - ifelse(pred.rf == "Spam", 1,0)))

```

On a une erreur de test de 6%.

## Question 7

Le modèle random forrest est le plus performant en terme d'erreur de test. Cependant, il est aussi le plus couteux en terme de temps de calcul, et n'est pas reproductible en pratique. Le modèle avec le bagging est assez performant en terme d'erreur de test, et ne demande pas un trop grand temps de calcul. Néanmoins, 100 arbres avec bagging ne font pas beaucoup mieux qu'un seul arbre. En conclusion si on doit manipuler une trop grande base de données d'email, la complexité est bien trop grande pour utiliser le random forest, nous préfèrerons le bagging même s'il n'y a pas un grand gain de performance avec le modèle Cart.


# Bootstrap 
## Question 8
Ici, on considère une variable aléatoire $Y$ qui suit une loi normal de moyenne $\theta$ et de variance 1. On souhaiterait estimer son espérance par estimateur du maximum de vraisemblance.
```{r}
y = rnorm(100,4,1)
```

On a pour la vraisemblance
\begin{align*}
L(y_1,\dots,y_n\mid \theta) &= \displaystyle \prod_{i=1}^n \frac{1}{\sqrt{2\pi}}e^{(y_i-\theta)^2/2}\\
&= \displaystyle \frac{1}{\sqrt{2\pi}^n}e^{\sum_{i=1}^n(y_i-\theta)^2/2}\\
\mbox{En passant au log}\\
\implies\\
log(L(y_1,\dots,y_n\mid \theta)) &= \displaystyle log(\frac{1}{\sqrt{2\pi}^n})  + \sum_{i=1}^n(y_i-\theta)^2/2
\end{align*}
Maximisier la vraisemblance revient à maximiser $\displaystyle \frac{1}{2}\sum_{i=1}^n(y_i-\theta)^2$.
On en déduit que l'EMV ici est $$\displaystyle \hat{\theta} = \frac{1}{n}\sum_{i=1}^ny_i.$$
Il coïncide avec l'estimateur des moments et avec la moyenne empirique. On va produire plusieurs valeurs de $\hat{\theta}$ :

```{r}
n <- 100
N <- 1000
set.seed(11)
theta_hat <- rowMeans(matrix(rnorm(N*n,4,1), N,n))
boxplot(theta_hat,col = 'darkseagreen3')
hist(theta_hat, ylab="Fréquence", xlab="Estimateur",main="Histograme de l'estimateur", col="orange",breaks = 40,probability=TRUE)
x <-seq(3.5, 4.5, le=200)
z <- 1/sqrt(n)
points(x,dnorm(x, 4, z), type="l", lwd = 2, col="red")
```

## Question 9
On se place maintenant dans un cas plus réaliste, c'est à dire qu'on ne dispose pas de nombreuses observations pour notre variable aléatoire. On se limite alors à un échantillon de taille $n=100$.
La méthode bootsrtap se prête particulièrement à ce cas. 
On tire aléatoirement $n\times N$ avec remise dans notre ensemble de réalisation $y$. pour obtenir $N$ éhcantillons de taille $n$. On prend alors la moyenne pour chaque échantillon.


```{r}
set.seed(11)
Y = y[sample(c(1:n),n*N,replace =T)] ## Replace =TRUE permet d'effectuer un tirage avec remise
Y_matrix = matrix(Y,byrow=TRUE, ncol = n) ## on a 1000 lignes avec 100 réalisations obtenues par tirage
theta_hat <- rowMeans(Y_matrix)
boxplot(theta_hat,col = 'mediumpurple1')
hist(theta_hat, ylab="Fréquence", xlab="Estimateur",main="Histograme par boostrap", col="tomato",breaks = 40,probability=TRUE)
x <-seq(3, 5, le=200)
z <- 1/sqrt(n)
points(x,dnorm(x, 4, z), type="l", lwd = 2, col="navy")
```

On peut remarquer que l'histograme obtenu par bootstrap est centré autour de 4.1 alors que le précédant l'est autour de 4. La méthode bootsrap a donc été assez précise et se rapproche d'une manière respectable de ce qu'on désirait. De plus on peut voir que la distribution des valeurs est quasiment symétrique. On remarque qu'il y a quelques valeurs qui dépasse le maximum dans les deux cas.
Grâce aux lignes que l'on a tracé, on se rend facilement compte de l'écart ou de l'erreur de notre approximation. L'histogramme est légèrement décalé vers les valeurs plus hautes contrairement à celui du premier cas qui se confond pratiquement avec sa ligne.

On aurait pu faire les mêmes observations depuis le box-plot. On voit que les valeurs obtenues par bootstrap sont centrées autour de 4.1 et qu'on la distribution est symétrique. Il y a un peu mois de valeurs abhérentes pour le bootsrap que pour le premier cas.


On peut conclure que notre approximtion a été correcte et que le réchantillonage dit bootstrap peut palier aux problème de manque de données. On aurait aussi pu démontrer que le bootstraping était utile lorsqu'on on ne connait pas du tout la distribution des observations et peut permettre d'identifier des intervalles de confiance mais ce n'était pas là l'objet de l'exercice.



# Autour de l’algorithme Adaboost

## Question 10


\begin{tabular}{|l|c|c|c|r|} \hline & &  \textbf{Perte}  & &  \\ & & \textbf{exponentielle} & \textbf{Perte binaire} & \\ $\hat{c}(x_*)$ & $\hat{y}_*$ & $exp(-y_* \hat{c}(x_*))$ &   $1(y_* \neq \hat{y}_*)$ & $y_*$ \tabularnewline \hline 0.3 & 1 & 1.35 & 1 & -1 \tabularnewline \hline -0.2 & -1 & 0.8187 & 0 & -1 \tabularnewline \hline 1.5 & 1 & 0.2231 & 0 & 1 \tabularnewline \hline -4.3 & -1 & 73.7 & 1 & 1 \tabularnewline \hline \end{tabular}

Si $y_*$ et $\hat{c}(x_*)$ sont de signe différents, c'est à dire $exp(-y_* \hat{c}(x_*)) > 1$, alors $1(y_* \neq \hat{y}_*) = 1$, et réciproquement. Donc en comparant  $exp(-y_* \hat{c}(x_*))$ à 1 on connait entièrement $1(y_* \neq \hat{y}_*)$, c'est à dire les erreurs. Mais en plus, $exp(-y_* \hat{c}(x_*))$ nous dit à quel point on se trompe. On le voit à la dernière ligne du tableau,  $exp(-y_* \hat{c}(x_*)) = 73.7$ alors que l'on se trompe de façon moins importante à la première ligne : $exp(-y_* \hat{c}(x_*)) = 1.35$, on n'est pas pas loin de détecter la bonne réponse. Cela peut être utilisé dans le cadre du boosting : à la prochaine étape, on va donner plus de poids aux individus ayant une plus grande perte exponentielle, pour corriger le modèle là où il fait le plus d'erreurs.


## Question 11

Il faut prendre tree_depth = 2 car AdaBoost.M1 exécute à chaque étapes des règles de classifcation faible à partir d'arbres à deux feuilles. De plus n_rounds = 50, car l'énoncé nous demande 50 itérations d’entraînement.

```{r}
ada.email = adaboost(as.matrix(email.tr[,-58]), ifelse(email.tr$Class == "Spam", 1, -1), tree_depth = 2, n_rounds = 50, verbose = FALSE,
  control = NULL)
```

Et l'erreur de test :

```{r}
pred.ada <- predict(ada.email, email.te)
mean(abs(ifelse(email.te$Class == "Spam", 1,0) - ifelse(pred.ada == 1, 1,0)))

```

On fait donc une erreur de 6.5 %.

## Question 12

Avec la library xgboost :

On reprend la classification d'email avec les mêmes données et la même partition. On opte pour un model de regression logistique binaire, en appliquant la règle de Bayes pour faire la prédiction. On peut ainsi évaluer l'erreur de test en fonction du nombre d'itérations dans le boosting :

```{r}
erreurtest_xgboost = function(N)
{
  xgb.email = xgboost(data = as.matrix(email.tr[,-58]), 
                    label = ifelse(email.tr$Class == "Spam", 1, 0),
                    objective = "binary:logistic",
                    booster = "gbtree",
                    nrounds = N,
                    verbose = FALSE)
  
  pred.xgb <- predict(xgb.email, as.matrix(email.te[,-58]))
  pred.xgb <- ifelse(pred.xgb >.5, 1, 0)
  return (mean(abs(ifelse(email.te$Class == "Spam", 1,0) - ifelse(pred.xgb == 1, 1,0))))
}

```

On affiche ainsi l'erreur de test en fonction du nombre d'itérations :

```{r echo= TRUE}
surajustement = sapply(seq(from = 1, to = 10000, by = 500), erreurtest_xgboost)

plot(seq(from = 1, to = 10000, by = 500), surajustement, 
     type = "l",
     xlab = "itérations",
     ylab = "erreur de test", 
     main = "Phénomène de sur-ajustement")
```

On voit bien le phénomène de sur-ajustement qui apparaît, l'erreur de test augmente car on veut trop coller aux données.







